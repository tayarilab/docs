# Engine summary

## What it is

The **engine** is a small web app that serves a **medical quiz** (Cardiovascular System). Each quiz has **5 multiple-choice questions** with **4 options** and **1 correct answer**. Questions are **generated by a language model** from chapter content stored in `engine/data/medical/cvs.json`. There are **no built-in fallback questions**: if the LLM cannot produce enough questions, the API returns an error.

## Components

| Component | Role |
|-----------|------|
| **quiz_server.py** | Flask app: serves `public/index.html`, exposes `GET /api/questions`, loads chapter from `cvs.json`, calls LLM (Ollama or embedded), manages question pool, returns random 5 with shuffled options. |
| **data/medical/cvs.json** | Chapter source. Must contain a `full_text` field (string) used as the only input for question generation. Optional `sections` for structure. |
| **public/index.html** | Quiz UI: fetches `/api/questions`, renders 5 questions, submits answers, shows score and correct/wrong. |
| **quiz_llm/** | Optional package: downloads a small GGUF model (e.g. SmolLM2-360M) from Hugging Face and generates questions without Ollama. Used when Ollama is unavailable or when `USE_EMBEDDED_LLM=1`. |
| **.quiz_pool.json** | Cache file (under `engine/`). Stores up to 20 LLM-generated questions; each request picks a random 5 from this pool. Created/updated when the pool is empty or too small. |
| **.quiz_models/** | Optional. When using embedded LLM with project cache, the downloaded GGUF model is stored here (or under `QUIZ_LLM_CACHE`). |

## Deployment modes

| Mode | When it is used | Requirement |
|------|------------------|-------------|
| **Ollama** | Default when the server starts (unless `USE_EMBEDDED_LLM=1`). Server calls `OLLAMA_HOST` (default `http://localhost:11434`) with `OLLAMA_QUIZ_MODEL` (default `llama3.2`). | Ollama installed and running; model pulled (e.g. `ollama run llama3.2`). |
| **Embedded (quiz_llm)** | When Ollama fails or when `USE_EMBEDDED_LLM=1`. Uses `llama-cpp-python` and a GGUF model (e.g. SmolLM2-360M) downloaded from Hugging Face. | `pip install -r requirements-llm.txt`; on Windows, a pre-built wheel or build tools may be needed. |
| **Error** | When `cvs.json` is missing or has no `full_text`, or when the LLM returns fewer than 5 questions. | Fix chapter file or ensure Ollama/embedded LLM is available and working. |

## Key constants (quiz_server.py)

- **QUIZ_SIZE** = 5 (questions per quiz).
- **POOL_SIZE_TARGET** = 20 (questions to generate and cache per LLM run).
- **Chapter text** sent to the LLM is truncated to the first 12,000 characters of `full_text`.

## Environment variables

| Variable | Default | Effect |
|----------|---------|--------|
| `OLLAMA_HOST` | `http://localhost:11434` | Base URL of the Ollama API. |
| `OLLAMA_QUIZ_MODEL` | `llama3.2` | Model name used for generation. |
| `USE_EMBEDDED_LLM` | (unset) | If `1`, `true`, or `yes`, use only the embedded model (no Ollama). |
| `QUIZ_LLM_CACHE` | `engine/.quiz_models` | Directory for the embedded GGUF model when using project cache. |

## Dependencies

- **Required:** `flask`, `requests` (`engine/requirements.txt`).
- **Optional (embedded LLM):** `llama-cpp-python`, `huggingface_hub` (`engine/requirements-llm.txt`).

## File layout (engine)

```
engine/
├── quiz_server.py       # Main server and API
├── requirements.txt
├── requirements-llm.txt
├── download_model.py   # Pre-download embedded model
├── data/medical/
│   └── cvs.json        # Chapter (full_text + optional sections)
├── public/
│   └── index.html      # Quiz UI
├── quiz_llm/           # Embedded LLM package
│   ├── __init__.py
│   ├── model_loader.py
│   └── generator.py
├── scripts/
│   └── md_to_json.py   # Optional: convert Markdown → cvs.json
├── .quiz_pool.json     # Cached questions (created at runtime)
└── .quiz_models/       # Optional: cached GGUF model
```

See [Workflow](workflow.md) for the end-to-end flow and [Question generation](question-generation.md) for how questions are produced and served.
